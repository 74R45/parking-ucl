{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "spMB7eTW8Hqh"
      },
      "source": [
        "# Master's Thesis on the topic of\n",
        "## \"Continual learning method for image classification in computer vision\"\n",
        "by Taras Kreshchenko"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSCyvwHIPkTi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "from enum import Enum\n",
        "\n",
        "import torch\n",
        "from torch import Tensor, nn, optim\n",
        "from torch.optim import Optimizer, SGD, lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, dataloader\n",
        "from torchvision import transforms as T\n",
        "from torchmetrics import Metric\n",
        "from torchmetrics.classification import BinaryAccuracy, BinaryAUROC\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Tuple, List, Dict, Callable, TypeAlias, TypeVar\n",
        "\n",
        "Transform: TypeAlias = Callable[[Image.Image], Tensor]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def now(format: str = \"%Y-%m-%d %H:%M:%S.%f\") -> str:\n",
        "  return datetime.now().strftime(format)[:-3]\n",
        "\n",
        "def log(value: str, end: str = '\\n') -> None:\n",
        "  print(f'{now()} | {value}', end=end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "131Ktg2gxsDK"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE.type"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zZDgbe7Pw-nH"
      },
      "source": [
        "## Downloading data & preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download the datasets if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIxgyRnyAw9-",
        "outputId": "a6dd4a8e-214a-4e04-fe97-a8da110fc850"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n",
        "\n",
        "!curl -Lo CNRPark.zip https://github.com/fabiocarrara/deep-parking/releases/download/archive/CNRPark-Patches-150x150.zip\n",
        "!mkdir data/CNRPark\n",
        "!unzip CNRPark.zip -d data/CNRPark\n",
        "!rm CNRPark.zip\n",
        "\n",
        "!curl -Lo CNR-EXT.zip https://github.com/fabiocarrara/deep-parking/releases/download/archive/CNR-EXT-Patches-150x150.zip\n",
        "!mkdir data/CNR-EXT\n",
        "!unzip CNR-EXT.zip -d data/CNR-EXT\n",
        "!rm CNR-EXT.zip\n",
        "\n",
        "!curl -LO https://www.inf.ufpr.br/vri/databases/PKLot.tar.gz\n",
        "!tar -xf PKLot.tar.gz -C data\n",
        "!rm PKLot.tar.gz\n",
        "\n",
        "!curl -LO https://github.com/fabiocarrara/deep-parking/releases/download/archive/splits.zip\n",
        "!unzip splits.zip -d data\n",
        "!rm splits.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DS(Enum):\n",
        "  CNRPark = 0\n",
        "  CNRParkExt = 1\n",
        "  PKLot = 2\n",
        "\n",
        "DS_PATHS = {\n",
        "  DS.CNRPark: 'data/CNRPark',\n",
        "  DS.CNRParkExt: 'data/CNR-EXT/PATCHES',\n",
        "  DS.PKLot: 'data/PKLot/PKLotSegmented'\n",
        "}\n",
        "SPLIT_PATHS = {\n",
        "  DS.CNRPark: 'data/splits/CNRParkAB',\n",
        "  DS.CNRParkExt: 'data/splits/CNRPark-EXT',\n",
        "  DS.PKLot: 'data/splits/PKLot'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DatasetSource:\n",
        "  def __init__(self, ds: DS, split_name: str) -> None:\n",
        "    self.ds = ds\n",
        "    self.ds_name = ds.name\n",
        "    self.split_name = split_name\n",
        "    self.ds_path = DS_PATHS[ds]\n",
        "    self.split_path = SPLIT_PATHS[ds] + '/' + split_name + '.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO369Nl2xaxq"
      },
      "outputs": [],
      "source": [
        "class ParkingDataset(Dataset[Tuple[Tensor, float]]):\n",
        "  def __init__(self, ds_source: DatasetSource, transform: Transform) -> None:\n",
        "    img_path = ds_source.ds_path\n",
        "    with open(ds_source.split_path, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "      # split files contain copies of a few images with (2) in their names,\n",
        "      # possibly caused by an accidental copy; exluding these from the dataset\n",
        "      data = [line.split() for line in lines if '(2)' not in line]\n",
        "      self.img_path_list = [os.path.join(img_path, row[0]) for row in data]\n",
        "      self.label_list = [float(row[1]) for row in data]\n",
        "      self.transform = transform\n",
        "      self.size = len(self.label_list)\n",
        "\n",
        "  def __getitem__(self, index: int) -> Tuple[Tensor, float]:\n",
        "    img_path = self.img_path_list[index]\n",
        "    img = Image.open(img_path)\n",
        "\n",
        "    tensor = self.transform(img)\n",
        "    label = self.label_list[index]\n",
        "\n",
        "    return tensor, label\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return self.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr2x2ocdzNpJ"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch: List[Tensor]) -> dataloader._collate_fn_t:\n",
        "  batch = list(filter(lambda x: x is not None, batch))\n",
        "  return dataloader.default_collate(batch)\n",
        "\n",
        "D = TypeVar('D')\n",
        "def create_data_loader(dataset: Dataset[D], batch_size: int = 1, shuffle: bool = False) -> DataLoader[D]:\n",
        "  # return DataLoader[D](dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
        "  return DataLoader[D](dataset, batch_size=batch_size, shuffle=shuffle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grayscale_to_rgb: Callable[[Tensor], Tensor] = \\\n",
        "  lambda tensor: tensor.repeat(3, 1, 1) if tensor.size(0) == 1 else tensor\n",
        "\n",
        "simple_transform = T.Compose([\n",
        "  T.Resize((256, 256)),\n",
        "  T.ToTensor(),\n",
        "  T.Lambda(grayscale_to_rgb)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_mean_and_std(ds_source: DatasetSource) -> tuple[Tensor, Tensor]:\n",
        "  ds = ParkingDataset(ds_source, transform=simple_transform)\n",
        "  means = Tensor([0., 0., 0.])\n",
        "  stds = Tensor([0., 0., 0.])\n",
        "\n",
        "  for i in range(len(ds)):\n",
        "    means += ds[i][0].mean([1, 2])\n",
        "  means /= len(ds)\n",
        "\n",
        "  for i in range(len(ds)):\n",
        "    im = ds[i][0]\n",
        "    for ch in range(3):\n",
        "      stds[ch] += ((im[ch, :, :] - means[ch])**2).sum() / (im.shape[1] * im.shape[2])\n",
        "  stds = (stds / len(ds)).sqrt()\n",
        "\n",
        "  return means, stds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_transform(mean: Tensor | List[float],\n",
        "                     std: Tensor | List[float],\n",
        "                     train: bool) -> Transform:\n",
        "  if train:\n",
        "    return T.Compose([\n",
        "      T.Resize((256, 256)),\n",
        "      T.RandomHorizontalFlip(),\n",
        "      T.RandomCrop(224),\n",
        "      T.ToTensor(),\n",
        "      T.Lambda(grayscale_to_rgb),\n",
        "      T.Normalize(mean, std)\n",
        "   ])\n",
        "  else:\n",
        "    return T.Compose([\n",
        "      T.Resize((224, 224)),\n",
        "      T.ToTensor(),\n",
        "      T.Lambda(grayscale_to_rgb),\n",
        "      T.Normalize(mean, std)\n",
        "   ])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-1RRenjReYsW"
      },
      "source": [
        "## Reproducing existing solutions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "041gMHcseg3h"
      },
      "source": [
        "### Amato, G. et al. Deep learning for decentralized parking lot occupancy detection. 2017"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlugFxVQJcAc"
      },
      "outputs": [],
      "source": [
        "class mAlexNet(nn.Module):\n",
        "  def __init__(self, num_classes: int = 2) -> None:\n",
        "    super().__init__()\n",
        "    self.input_channels = 3\n",
        "    self.num_output = num_classes\n",
        "    self.conv1 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=self.input_channels, out_channels=16, kernel_size=11, stride=4),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.LocalResponseNorm(5, k=2),\n",
        "      nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=16, out_channels=20, kernel_size=5, stride=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.LocalResponseNorm(5, k=2),\n",
        "      nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "    )\n",
        "    self.conv3 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=20, out_channels=30, kernel_size=3, stride=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "    )\n",
        "    self.fc1 = nn.Sequential(\n",
        "      nn.Linear(30*3*3, out_features=48),\n",
        "      nn.ReLU(inplace=True)\n",
        "    )\n",
        "    self.fc2 = nn.Linear(in_features=48, out_features=2)\n",
        "\n",
        "    self.conv1.apply(self.__init_weights)\n",
        "    self.conv2.apply(self.__init_weights)\n",
        "    self.conv3.apply(self.__init_weights)\n",
        "    self.fc1.apply(self.__init_weights)\n",
        "    self.__init_weights(self.fc2)\n",
        "    # nn.init.constant_(self.fc2.bias, 0)\n",
        "\n",
        "  def __init_weights(self, layer: nn.Module) -> None:\n",
        "    if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "      nn.init.xavier_uniform_(layer.weight)\n",
        "      # if layer.bias is not None:\n",
        "      #   nn.init.constant_(layer.bias, 1)\n",
        "\n",
        "  def get_features(self, x: Tensor) -> Tensor:\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc1(x)\n",
        "    return x\n",
        "  \n",
        "  def get_classes(self, x: Tensor) -> Tensor:\n",
        "    x = self.fc2(x)\n",
        "    x = F.softmax(x, dim=1)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x: Tensor) -> Tensor:\n",
        "    x = self.get_features(x)\n",
        "    x = self.get_classes(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr_mults = {\n",
        "    'conv1.0.weight': 1,\n",
        "    'conv1.0.bias': 2,\n",
        "    'conv2.0.weight': 1,\n",
        "    'conv2.0.bias': 2,\n",
        "    'conv3.0.weight': 1,\n",
        "    'conv3.0.bias': 2,\n",
        "    'fc1.0.weight': 1,\n",
        "    'fc1.0.bias': 2,\n",
        "    'fc2.weight': 1,\n",
        "    'fc2.bias': 2\n",
        "}\n",
        "\n",
        "decay_mults = {\n",
        "    'conv1.0.weight': 1,\n",
        "    'conv1.0.bias': 0,\n",
        "    'conv2.0.weight': 1,\n",
        "    'conv2.0.bias': 0,\n",
        "    'conv3.0.weight': 1,\n",
        "    'conv3.0.bias': 0,\n",
        "    'fc1.0.weight': 1,\n",
        "    'fc1.0.bias': 1,\n",
        "    'fc2.weight': 1,\n",
        "    'fc2.bias': 1\n",
        "}\n",
        "\n",
        "def create_malexnet_optimiser(model: nn.Module,\n",
        "                              lr: float,\n",
        "                              weight_decay: float = 0.,\n",
        "                              momentum: float = 0.) -> Optimizer:\n",
        "  param_groups: List[Dict[str, List[nn.Parameter] | float]] = []\n",
        "  for name, parameter in model.named_parameters():\n",
        "    param_groups.append({\n",
        "      'params': [parameter],\n",
        "      'lr': lr * lr_mults[name],\n",
        "      'weight_decay': weight_decay * decay_mults[name]\n",
        "    })\n",
        "  \n",
        "  optimiser = SGD(param_groups, lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
        "  return optimiser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_single_epoch(\n",
        "    model: nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    optimiser: Optimizer,\n",
        "    loss_fn: Callable[[Tensor, Tensor], Tensor],\n",
        "    metric_fns: List[Metric]\n",
        ") -> Tuple[float, List[float]]:\n",
        "  model.train()\n",
        "  running_loss = 0.\n",
        "\n",
        "  for input, target in data_loader:\n",
        "    input: Tensor = input.to(DEVICE)\n",
        "    target: Tensor = target.to(DEVICE)\n",
        "    optimiser.zero_grad()\n",
        "    \n",
        "    output: Tensor = model(input)\n",
        "    # since it's binary classification, we can discard one of the probabilities\n",
        "    output = output[:, 0]\n",
        "    loss: Tensor = loss_fn(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    for m in metric_fns:\n",
        "      m(output, target)\n",
        "  \n",
        "  running_loss /= len(data_loader)\n",
        "  metrics = [m.compute() for m in metric_fns]\n",
        "  for m in metric_fns:\n",
        "    m.reset()\n",
        "\n",
        "  return (running_loss, metrics)\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    loss_fn: Callable[[Tensor, Tensor], Tensor],\n",
        "    metric_fns: List[Metric]\n",
        ") -> Tuple[float, List[float]]:\n",
        "  model.eval()\n",
        "  running_loss = 0.\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for input, target in data_loader: \n",
        "      input: Tensor = input.to(DEVICE)\n",
        "      target: Tensor = target.to(DEVICE)\n",
        "\n",
        "      output: Tensor = model(input)\n",
        "      # since it's binary classification, we can discard one of the probabilities\n",
        "      output = output[:, 0]\n",
        "\n",
        "      running_loss += loss_fn(output, target).item()\n",
        "      for m in metric_fns:\n",
        "        m(output, target)\n",
        "\n",
        "  running_loss /= len(data_loader)\n",
        "  metrics = [m.compute() for m in metric_fns]\n",
        "  for m in metric_fns:\n",
        "    m.reset()\n",
        "  \n",
        "  return (running_loss, metrics)\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          train_data_loader: DataLoader,\n",
        "          val_data_loader: DataLoader | None,\n",
        "          optimiser: Optimizer,\n",
        "          lr_scheduler: lr_scheduler.LRScheduler,\n",
        "          loss_fn: Callable[[Tensor, Tensor], Tensor],\n",
        "          metric_fns: List[Metric],\n",
        "          epochs: int) -> None:\n",
        "  print_metrics: Callable[[List[float]], str] = \\\n",
        "    lambda metrics: ' | '.join([f'{type(fn).__name__}: {m:.4f}' for fn, m in zip(metric_fns, metrics)])\n",
        "\n",
        "  for i in range(epochs):\n",
        "    log(f'Epoch {i+1}')\n",
        "\n",
        "    loss, metrics = train_single_epoch(model, train_data_loader, optimiser, loss_fn, metric_fns)\n",
        "    log(f'Train | Loss: {loss:.4f} | {print_metrics(metrics)}')\n",
        "\n",
        "    if val_data_loader is not None:\n",
        "      loss, metrics = evaluate(model, val_data_loader, loss_fn, metric_fns)\n",
        "      log(f'Validation  | Loss: {loss:.4f} | {print_metrics(metrics)}')\n",
        "      \n",
        "    lr_scheduler.step()\n",
        "  log('Finished training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment(train_ds_source: DatasetSource,\n",
        "                   val_ds_source: DatasetSource | None,\n",
        "                   test_ds_sources: List[DatasetSource] | DatasetSource | None,\n",
        "                   epochs: int,\n",
        "                   lr: float,\n",
        "                   weight_decay: float,\n",
        "                   gamma: float,\n",
        "                   step_size: int,\n",
        "                   mean: List[float] | None = None,\n",
        "                   std: List[float] | None = None,\n",
        "                   shuffle: bool = False,\n",
        "                   save_path: str | None = None) -> nn.Module:\n",
        "  model = mAlexNet().to(DEVICE)\n",
        "  loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
        "  metric_fns = [BinaryAccuracy().to(DEVICE), BinaryAUROC().to(DEVICE)]\n",
        "  optimiser = create_malexnet_optimiser(model, lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
        "  scheduler = lr_scheduler.StepLR(optimiser, step_size=step_size, gamma=gamma)\n",
        "\n",
        "  mean_ = mean\n",
        "  std_ = std\n",
        "  if mean_ is None or std_ is None:\n",
        "    log(f'Calculating mean and std for {train_ds_source.ds_name}/{train_ds_source.split_name}')\n",
        "    mean_, std_ = get_mean_and_std(train_ds_source)\n",
        "    log(f'Mean: {mean_.tolist()}')\n",
        "    log(f'Std: {std_.tolist()}\\n')\n",
        "  train_transform = create_transform(mean_, std_, train=True)\n",
        "  test_transform = create_transform(mean_, std_, train=False)\n",
        "\n",
        "  train_dataset = ParkingDataset(train_ds_source, train_transform)\n",
        "  train_loader = create_data_loader(train_dataset, batch_size=64, shuffle=shuffle)\n",
        "  log(f'Training on {train_ds_source.ds_name}/{train_ds_source.split_name}', end='')\n",
        "\n",
        "  if val_ds_source is not None:\n",
        "    val_dataset = ParkingDataset(val_ds_source, test_transform)\n",
        "    val_loader = create_data_loader(val_dataset, batch_size=64)\n",
        "    print(f', validating on {val_ds_source.ds_name}/{val_ds_source.split_name}')\n",
        "  else:\n",
        "    val_loader = None\n",
        "    print()\n",
        "\n",
        "  train(model, train_loader, val_loader, optimiser, scheduler, loss_fn, metric_fns, epochs)\n",
        "\n",
        "  if save_path:\n",
        "    torch.save(model.state_dict(), 'models/' + save_path)\n",
        "\n",
        "  if test_ds_sources is not None:\n",
        "    test_ds_sources = test_ds_sources if isinstance(test_ds_sources, List) else [test_ds_sources]\n",
        "    for ds_source in test_ds_sources:\n",
        "      test_dataset = ParkingDataset(ds_source, test_transform)\n",
        "      test_loader = create_data_loader(test_dataset, batch_size=64)\n",
        "      log(f'Testing on {ds_source.ds_name}/{ds_source.split_name}')\n",
        "      loss, metrics = evaluate(model, test_loader, loss_fn, metric_fns)\n",
        "      metrics_str = ' | '.join([f'{type(fn).__name__}: {m:.4f}' for fn, m in zip(metric_fns, metrics)])\n",
        "      log(f'Loss: {loss:.4f} | {metrics_str}')\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model: nn.Module,\n",
        "               ds_sources: List[DatasetSource] | DatasetSource,\n",
        "               mean: List[float],\n",
        "               std: List[float]) -> None:\n",
        "  loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
        "  metric_fns = [BinaryAccuracy().to(DEVICE), BinaryAUROC().to(DEVICE)]\n",
        "  transform = create_transform(mean, std, train=False)\n",
        "\n",
        "  ds_sources = ds_sources if isinstance(ds_sources, List) else [ds_sources]\n",
        "  for test_ds_source in ds_sources:\n",
        "    test_dataset = ParkingDataset(test_ds_source, transform)\n",
        "    test_loader = create_data_loader(test_dataset, batch_size=64)\n",
        "    log(f'Testing on {test_ds_source.ds_name}/{test_ds_source.split_name}')\n",
        "    loss, metrics = evaluate(model, test_loader, loss_fn, metric_fns)\n",
        "    metrics_str = ' | '.join([f'{type(fn).__name__}: {m:.4f}' for fn, m in zip(metric_fns, metrics)])\n",
        "    log(f'Loss: {loss:.4f} | {metrics_str}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Running the experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_cnr_even = run_experiment(\n",
        "  DatasetSource(DS.CNRPark, 'even'), DatasetSource(DS.CNRPark, 'odd'), None,\n",
        "  epochs=18, lr=0.0001, weight_decay=0.0005, gamma=0.5, step_size=6,\n",
        "  mean=[0.4422, 0.4524, 0.3867], std=[0.1783, 0.1732, 0.1743])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_cnr_odd = run_experiment(\n",
        "  DatasetSource(DS.CNRPark, 'odd'), DatasetSource(DS.CNRPark, 'even'), None,\n",
        "  epochs=18, lr=0.0001, weight_decay=0.0005, gamma=0.5, step_size=6,\n",
        "  mean=[0.4353, 0.445, 0.3782], std=[0.1881, 0.1817, 0.1813])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_cnr_all = run_experiment(\n",
        "  DatasetSource(DS.CNRPark, 'all'),\n",
        "  None,\n",
        "  [DatasetSource(DS.CNRParkExt, 'test'), DatasetSource(DS.PKLot, 'twodays')],\n",
        "  epochs=6, lr=0.0008, weight_decay=0.0005, gamma=0.5, step_size=2,\n",
        "  mean=[0.4387, 0.4486, 0.3823], std=[0.1834, 0.1776, 0.178])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_pklot_train = run_experiment(\n",
        "  DatasetSource(DS.PKLot, 'train'),\n",
        "  DatasetSource(DS.CNRPark, 'all'),\n",
        "  [DatasetSource(DS.CNRParkExt, 'test'), DatasetSource(DS.PKLot, 'test')],\n",
        "  epochs=6, lr=0.0008, weight_decay=0.0005, gamma=0.5, step_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds_source = DatasetSource(DS.CNRParkExt, 'camera1')\n",
        "val_ds_source = DatasetSource(DS.CNRParkExt, 'camera5')\n",
        "test_ds_sources = [\n",
        "    DatasetSource(DS.CNRParkExt, 'camera2'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera3'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera4'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera5'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera6'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera7'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera8'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera9'),\n",
        "    DatasetSource(DS.PKLot, 'test')\n",
        "]\n",
        "model_cnrext_c1 = run_experiment(\n",
        "  train_ds_source, val_ds_source, test_ds_sources,\n",
        "  epochs=6, lr=0.0008, weight_decay=0.0005, gamma=0.75, step_size=2,\n",
        "  save_path='cnrext_c1_malexnet_v2.pth'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_cnrext_c8 = mAlexNet().to(DEVICE)\n",
        "model_cnrext_c8.load_state_dict(torch.load('models/cnrext_c8_malexnet.pth', map_location=DEVICE))\n",
        "ds_sources = [\n",
        "    DatasetSource(DS.CNRParkExt, 'camera1'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera2'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera3'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera4'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera5'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera6'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera7'),\n",
        "    DatasetSource(DS.CNRParkExt, 'camera9'),\n",
        "    DatasetSource(DS.PKLot, 'all')\n",
        "]\n",
        "test_model(model_cnrext_c8,\n",
        "           ds_sources,\n",
        "           mean=[0.4026, 0.3916, 0.3482],\n",
        "           std=[0.1811, 0.176, 0.1854])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fSmIpzb4f--J"
      },
      "source": [
        "## Applying Continual Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementing UCL-GV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from avalanche.benchmarks.utils.data_loader import ReplayDataLoader\n",
        "from avalanche.core import SupervisedPlugin\n",
        "from avalanche.training.supervised import Naive\n",
        "from avalanche.training.templates import SupervisedTemplate\n",
        "from avalanche.training.storage_policy import ReservoirSamplingBuffer\n",
        "from avalanche.training.utils import get_last_fc_layer\n",
        "from avalanche.benchmarks.utils import FilelistDataset, default_flist_reader\n",
        "from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_from_datasets\n",
        "from torch_kmeans import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UCLGVLoss(nn.Module):\n",
        "  def __init__(self, gamma1: float, gamma2: float, gamma3: float) -> None:\n",
        "    super().__init__()\n",
        "    self.l_ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    self.gamma1 = nn.Parameter(gamma1)\n",
        "    self.gamma2 = nn.Parameter(gamma2)\n",
        "    self.gamma3 = nn.Parameter(gamma3)\n",
        "\n",
        "  def l_pc(self, x: Tensor, pseudo_y: Tensor) -> Tensor:\n",
        "    x_norm = F.normalize(x)\n",
        "    y_norm = F.normalize(pseudo_y)\n",
        "    loss = - torch.log(torch.exp(x_norm) / (torch.exp(x_norm * y_norm) + torch.exp(x_norm * (1 - y_norm))))\n",
        "    return loss\n",
        "  \n",
        "  def l_ent(self, x: Tensor) -> Tensor:\n",
        "    entropy = -x * torch.log(x + 1e-5)\n",
        "    entropy = torch.sum(entropy, dim=1)\n",
        "    return entropy \n",
        "\n",
        "  def forward(self, x: Tensor, pseudo_y: Tensor) -> Tensor:\n",
        "    return self.gamma1 * self.l_ent(x) + \\\n",
        "      self.gamma2 * self.l_ce(x, pseudo_y) + \\\n",
        "      self.gamma3 * self.l_pc(x, pseudo_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UCLGVPlugin(SupervisedPlugin):\n",
        "  def __init__(self, mem_size: int, gamma1: float, gamma2: float, gamma3: float) -> None:\n",
        "    super().__init__()\n",
        "    self.buffer = ReservoirSamplingBuffer(max_size=mem_size)\n",
        "    self.kmeans = KMeans(n_clusters=2)\n",
        "    self.loss = UCLGVLoss(gamma1, gamma2, gamma3)\n",
        "\n",
        "  def before_eval_exp(self, strategy: SupervisedTemplate,\n",
        "                      num_workers: int = 0, shuffle: bool = False,\n",
        "                      *args, **kwargs) -> None:\n",
        "    \"\"\" Use a custom dataloader to combine samples from the current data and memory buffer. \"\"\"\n",
        "    if len(self.buffer.buffer) > 0:\n",
        "      strategy.dataloader = ReplayDataLoader(\n",
        "        strategy.adapted_dataset, # type: ignore\n",
        "        self.buffer.buffer,\n",
        "        num_workers=num_workers,\n",
        "        batch_size=strategy.train_mb_size,\n",
        "        shuffle=shuffle)\n",
        "\n",
        "  def after_eval_forward(self, strategy: SupervisedTemplate, *args, **kwargs) -> None:\n",
        "    \"\"\" Calculate loss, adapt the feature extractor. \"\"\"\n",
        "    strategy.model.train()\n",
        "    _, classifier = get_last_fc_layer(strategy.model)\n",
        "    classifier.eval()\n",
        "\n",
        "    assert(callable(strategy.model.get_features))\n",
        "    features = strategy.model.get_features(strategy.mbatch)\n",
        "    pseudo_labels = self.kmeans(features)\n",
        "    \n",
        "    loss = self.loss(features, pseudo_labels)\n",
        "    loss.backward()\n",
        "\n",
        "    strategy.model.eval()\n",
        "\n",
        "  def after_eval_exp(self, strategy: SupervisedTemplate, *args, **kwargs) -> None:\n",
        "    \"\"\" Update the buffer. \"\"\"\n",
        "    self.buffer.update(strategy, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ordered_flist_reader(flist: str) -> List[Tuple[str, int]]:\n",
        "  res = default_flist_reader(flist)\n",
        "  res.sort(key=(lambda path: path[-28:-12]))\n",
        "  return res\n",
        "\n",
        "def create_avalanche_dataset(ds_source: DatasetSource) -> FilelistDataset:\n",
        "  return FilelistDataset(\n",
        "    ds_source.ds_path,\n",
        "    ds_source.split_path,\n",
        "    create_transform(mean=[0.4353, 0.445, 0.3782], std=[0.1881, 0.1817, 0.1813], train=True),\n",
        "    create_transform(mean=[0.4353, 0.445, 0.3782], std=[0.1881, 0.1817, 0.1813], train=False),\n",
        "    ordered_flist_reader\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnrpark_all      = create_avalanche_dataset(DatasetSource(DS.CNRPark, 'all'))\n",
        "cnrext_c1        = create_avalanche_dataset(DatasetSource(DS.CNRParkExt, 'camera1'))\n",
        "cnrext_c2        = create_avalanche_dataset(DatasetSource(DS.CNRParkExt, 'camera2'))\n",
        "cnrext_c3        = create_avalanche_dataset(DatasetSource(DS.CNRParkExt, 'camera3'))\n",
        "cnrext_c4        = create_avalanche_dataset(DatasetSource(DS.CNRParkExt, 'camera4'))\n",
        "cnrext_c5        = create_avalanche_dataset(DatasetSource(DS.CNRParkExt, 'camera5'))\n",
        "cnrext_c6        = create_avalanche_dataset(DatasetSource(DS.CNRParkExt, 'camera6'))\n",
        "cnrext_c7        = create_avalanche_dataset(DatasetSource(DS.CNRParkExt, 'camera7'))\n",
        "cnrext_c8        = create_avalanche_dataset(DatasetSource(DS.CNRParkExt, 'camera8'))\n",
        "cnrext_c9        = create_avalanche_dataset(DatasetSource(DS.CNRParkExt, 'camera9'))\n",
        "cnrext_all       = create_avalanche_dataset(DatasetSource(DS.CNRParkExt, 'all'))\n",
        "pklot_twodays    = create_avalanche_dataset(DatasetSource(DS.PKLot, 'twodays'))\n",
        "pklot_nottwodays = create_avalanche_dataset(DatasetSource(DS.PKLot, 'nottwodays'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_cl_experiment(train_ds: List[FilelistDataset] | FilelistDataset,\n",
        "                      test_ds: FilelistDataset,\n",
        "                      lr: float,\n",
        "                      weight_decay: float,\n",
        "                      uclgv_params: Tuple[float, float, float] | None = None,\n",
        "                      save_path: str | None = None) -> nn.Module:\n",
        "  train_ds_list = train_ds if isinstance(train_ds, List) else [train_ds]\n",
        "  test_ds_list = [test_ds]\n",
        "  benchmark = benchmark_from_datasets(train=train_ds_list, test=test_ds_list)\n",
        "  plugins = \\\n",
        "    [] if uclgv_params is None \\\n",
        "    else [UCLGVPlugin(mem_size=2000, gamma1=uclgv_params[0], gamma2=uclgv_params[1], gamma3=uclgv_params[2])]\n",
        "  model = mAlexNet().to(DEVICE)\n",
        "  strategy = Naive(\n",
        "    model=model,\n",
        "    optimizer=create_malexnet_optimiser(model, lr=lr, weight_decay=weight_decay, momentum=0.9),\n",
        "    criterion=nn.CrossEntropyLoss().to(DEVICE),\n",
        "    train_mb_size=64,\n",
        "    plugins=plugins)\n",
        "\n",
        "  for experience in benchmark.train_stream:\n",
        "    print('Start of experience: ', experience.current_experience)\n",
        "    strategy.train(experience)\n",
        "    print('Training completed.')\n",
        "\n",
        "  if save_path:\n",
        "    torch.save(model.state_dict(), 'models/' + save_path)\n",
        "\n",
        "  print('Computing accuracy on the whole test set.')\n",
        "  strategy.eval(benchmark.test_stream)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Running the experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_cl_experiment(\n",
        "  cnrext_all, cnrpark_all,\n",
        "  lr=0.0008, weight_decay=0.0005, uclgv_params=(1, 0.1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_cl_experiment(\n",
        "  cnrext_all, cnrpark_all,\n",
        "  lr=0.0008, weight_decay=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_cl_experiment(\n",
        "  [cnrext_c1, cnrext_c2, cnrext_c3, cnrext_c4, cnrext_c5, cnrext_c6, cnrext_c7, cnrext_c8], cnrext_c9,\n",
        "  lr=0.0008, weight_decay=0.0005, uclgv_params=(1, 0.1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_cl_experiment(\n",
        "  [cnrext_c1, cnrext_c2, cnrext_c3, cnrext_c4, cnrext_c5, cnrext_c6, cnrext_c7, cnrext_c8], cnrext_c9,\n",
        "  lr=0.0008, weight_decay=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_cl_experiment(\n",
        "  pklot_nottwodays, pklot_twodays,\n",
        "  lr=0.0008, weight_decay=0.0005, uclgv_params=(1, 0.1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_cl_experiment(\n",
        "  pklot_nottwodays, pklot_twodays,\n",
        "  lr=0.0008, weight_decay=0.0005)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
